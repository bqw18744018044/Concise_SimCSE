{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a61cde9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from abc import ABC\n",
    "from tqdm.notebook import tqdm\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Union, Optional, Dict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, TrainingArguments, Trainer\n",
    "from transformers.models.bert.modeling_bert import BertPreTrainedModel, BertModel\n",
    "from transformers.tokenization_utils_base import PaddingStrategy, PreTrainedTokenizerBase\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput, BaseModelOutputWithPoolingAndCrossAttentions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1b092c",
   "metadata": {},
   "source": [
    "## 一、定义参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ea34dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataArguments:\n",
    "    train_file: str = field(default=\"./data/simcse/wiki1m_for_simcse.txt\",\n",
    "                            metadata={\"help\": \"The path of train file\"})\n",
    "    model_name_or_path: str = field(default=\"E:/pretrained/bert-base-uncased\",\n",
    "                                    metadata={\"help\": \"The name or path of pre-trained language model\"})\n",
    "    max_seq_length: int = field(default=32,\n",
    "                                metadata={\"help\": \"The maximum total input sequence length after tokenization.\"})\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "        output_dir=\"./checkpoints\",\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=64,\n",
    "        learning_rate=3e-5,\n",
    "        load_best_model_at_end=True,\n",
    "        overwrite_output_dir=True,\n",
    "        do_train=True,\n",
    "        do_eval=False,\n",
    "        logging_steps=10)\n",
    "\n",
    "\n",
    "data_args = DataArguments()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975d417e",
   "metadata": {},
   "source": [
    "## 二、读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a7af418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64a01a7d88434f8089d389eefd04ca1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=39.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<class 'list'>\n",
      "YMCA in South Australia\n"
     ]
    }
   ],
   "source": [
    "# 初始化tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(data_args.model_name_or_path)\n",
    "# 读取训练数据\n",
    "with open(data_args.train_file, encoding=\"utf8\") as file:\n",
    "    texts = [line.strip() for line in tqdm(file.readlines())]\n",
    "print(type(texts))\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4851a6a",
   "metadata": {},
   "source": [
    "## 三、构建Dataset和collate_fn\n",
    "### 3.1 构建Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5f62fb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2fe26f3e5954923b7d09beb49414a03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=39.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "306e00fecfd144ad881afbe99126813c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=39.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a674c2ff67cd42f9a57b0d466a87e23d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=39.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'input_ids': [[101, 26866, 1999, 2148, 2660, 102], [101, 26866, 1999, 2148, 2660, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]], 'token_type_ids': [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]]}\n"
     ]
    }
   ],
   "source": [
    "class PairDataset(Dataset):\n",
    "    def __init__(self, examples: List[str]):\n",
    "        total = len(examples)\n",
    "        # 将所有样本复制一份用于对比学习\n",
    "        sentences_pair = examples + examples\n",
    "        sent_features = tokenizer(sentences_pair,\n",
    "                                  max_length=data_args.max_seq_length,\n",
    "                                  truncation=True,\n",
    "                                  padding=False)\n",
    "        features = {}\n",
    "        # 将相同的样本放在同一个列表中\n",
    "        for key in sent_features:\n",
    "            features[key] = [[sent_features[key][i], sent_features[key][i + total]] for i in tqdm(range(total))]\n",
    "        self.input_ids = features[\"input_ids\"]\n",
    "        self.attention_mask = features[\"attention_mask\"]\n",
    "        self.token_type_ids = features[\"token_type_ids\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[item],\n",
    "            \"attention_mask\": self.attention_mask[item],\n",
    "            \"token_type_ids\": self.token_type_ids[item]\n",
    "        }\n",
    "\n",
    "train_dataset = PairDataset(texts)\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ddd0d9",
   "metadata": {},
   "source": [
    "### 3.2 构建collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d212a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'token_type_ids'])\n",
      "torch.Size([4, 2, 32])\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class DataCollator:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, List[int]]]) -> Dict[str, torch.Tensor]:\n",
    "        special_keys = ['input_ids', 'attention_mask', 'token_type_ids']\n",
    "        batch_size = len(features)\n",
    "        if batch_size == 0:\n",
    "            return\n",
    "        # flat_features: [sen1, sen1, sen2, sen2, ...]\n",
    "        flat_features = []\n",
    "        for feature in features:\n",
    "            for i in range(2):\n",
    "                flat_features.append({k: feature[k][i] for k in feature.keys() if k in special_keys})\n",
    "        # padding\n",
    "        batch = self.tokenizer.pad(\n",
    "            flat_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        # batch_size, 2, seq_len\n",
    "        batch = {k: batch[k].view(batch_size, 2, -1) for k in batch if k in special_keys}\n",
    "        return batch\n",
    "\n",
    "collate_fn = DataCollator(tokenizer)\n",
    "dataloader = DataLoader(train_dataset, batch_size=4, collate_fn=collate_fn)\n",
    "batch = next(iter(dataloader))\n",
    "print(batch.keys())\n",
    "print(batch[\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef5014a",
   "metadata": {},
   "source": [
    "## 四、构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "059f6ff4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at E:/pretrained/bert-base-uncased were not used when initializing BertForCL: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForCL from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForCL from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForCL were not initialized from the model checkpoint at E:/pretrained/bert-base-uncased and are newly initialized: ['mlp.dense.weight', 'mlp.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['loss', 'logits'])\n"
     ]
    }
   ],
   "source": [
    "# 全连接层，用于投影CLS的向量表示\n",
    "class MLPLayer(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(input_size, output_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = self.dense(features)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "# 相似度层，计算向量间相似度\n",
    "class Similarity(nn.Module):\n",
    "    def __init__(self, temp):\n",
    "        super().__init__()\n",
    "        self.temp = temp\n",
    "        self.cos = nn.CosineSimilarity(dim=-1)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        return self.cos(x, y) / self.temp\n",
    "\n",
    "    \n",
    "# SimCSE的完整模型结构\n",
    "class BertForCL(BertPreTrainedModel, ABC):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.bert = BertModel(config)\n",
    "        self.mlp = MLPLayer(config.hidden_size, config.hidden_size)\n",
    "        self.sim = Similarity(temp=0.05)\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids=None,\n",
    "                attention_mask=None,\n",
    "                token_type_ids=None,\n",
    "                position_ids=None,\n",
    "                head_mask=None,\n",
    "                inputs_embeds=None,\n",
    "                labels=None,\n",
    "                output_attentions=None,\n",
    "                output_hidden_states=None,\n",
    "                return_dict=None,\n",
    "                sent_emb=False):\n",
    "        if sent_emb:\n",
    "            # 模型推断时使用的forward\n",
    "            return self.sentemb_forward(input_ids=input_ids,\n",
    "                                        attention_mask=attention_mask,\n",
    "                                        token_type_ids=token_type_ids,\n",
    "                                        position_ids=position_ids,\n",
    "                                        head_mask=head_mask,\n",
    "                                        inputs_embeds=inputs_embeds,\n",
    "                                        labels=labels,\n",
    "                                        output_attentions=output_attentions,\n",
    "                                        output_hidden_states=output_hidden_states,\n",
    "                                        return_dict=return_dict)\n",
    "        else:\n",
    "            # 模型训练时使用的forward\n",
    "            return self.cl_forward(input_ids=input_ids,\n",
    "                                   attention_mask=attention_mask,\n",
    "                                   token_type_ids=token_type_ids,\n",
    "                                   position_ids=position_ids,\n",
    "                                   head_mask=head_mask,\n",
    "                                   inputs_embeds=inputs_embeds,\n",
    "                                   labels=labels,\n",
    "                                   output_attentions=output_attentions,\n",
    "                                   output_hidden_states=output_hidden_states,\n",
    "                                   return_dict=return_dict)\n",
    "\n",
    "    def sentemb_forward(self,\n",
    "                        input_ids=None,\n",
    "                        attention_mask=None,\n",
    "                        token_type_ids=None,\n",
    "                        position_ids=None,\n",
    "                        head_mask=None,\n",
    "                        inputs_embeds=None,\n",
    "                        labels=None,\n",
    "                        output_attentions=None,\n",
    "                        output_hidden_states=None,\n",
    "                        return_dict=None):\n",
    "        # 1.使用bert进行编码\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        # 2.取cls的表示\n",
    "        cls_output = outputs.last_hidden_state[:, 0]\n",
    "        # 3.使用MLP进行投影\n",
    "        cls_output = self.mlp(cls_output)\n",
    "        # 返回\n",
    "        if not return_dict:\n",
    "            return (outputs[0], cls_output) + outputs[2:]\n",
    "\n",
    "        return BaseModelOutputWithPoolingAndCrossAttentions(\n",
    "            pooler_output=cls_output,\n",
    "            last_hidden_state=outputs.last_hidden_state,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "        )\n",
    "\n",
    "    def cl_forward(self,\n",
    "                   input_ids=None,\n",
    "                   attention_mask=None,\n",
    "                   token_type_ids=None,\n",
    "                   position_ids=None,\n",
    "                   head_mask=None,\n",
    "                   inputs_embeds=None,\n",
    "                   labels=None,\n",
    "                   output_attentions=None,\n",
    "                   output_hidden_states=None,\n",
    "                   return_dict=None):\n",
    "        # input_ids: batch_size, num_sent, len\n",
    "        batch_size = input_ids.size(0)\n",
    "        num_sent = input_ids.size(1)  # 2\n",
    "        # 1. 重塑输入张量的形状，使其满足bert对输入的要求\n",
    "        # input_ids: batch_size * num_sent, len\n",
    "        input_ids = input_ids.view((-1, input_ids.size(-1)))\n",
    "        attention_mask = attention_mask.view((-1, attention_mask.size(-1)))\n",
    "        # 2. 使用bert进行编码\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        # 3. 取cls的向量表示\n",
    "        cls_output = outputs.last_hidden_state[:, 0]\n",
    "        # 4. 重塑形状\n",
    "        cls_output = cls_output.view((batch_size, num_sent, cls_output.size(-1)))\n",
    "        # 5. 全连接层投影\n",
    "        # batch_size, num_sent, 768\n",
    "        cls_output = self.mlp(cls_output)\n",
    "        # 6. 将同一批样本的两次向量表示分开\n",
    "        z1, z2 = cls_output[:, 0], cls_output[:, 1]\n",
    "        # 7. 计算两两相似度，得到相似度矩阵cos_sim\n",
    "        cos_sim = self.sim(z1.unsqueeze(1), z2.unsqueeze(0))\n",
    "        # 8. 生成标签[0,1,...,batch_size-1]，该标签用于提高相似度句子cos_sim对角线，并降低非对角线\n",
    "        labels = torch.arange(cos_sim.size(0)).long().to(self.device)\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(cos_sim, labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (cos_sim,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=cos_sim,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "model = BertForCL.from_pretrained(data_args.model_name_or_path)\n",
    "cl_out = model(**batch, return_dict=True)\n",
    "print(cl_out.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87631cf9",
   "metadata": {},
   "source": [
    "## 五、模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae435624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))\n",
    "trainer = Trainer(model=model,\n",
    "                  train_dataset=train_dataset,\n",
    "                  args=training_args,\n",
    "                  tokenizer=tokenizer,\n",
    "                  data_collator=collate_fn)\n",
    "trainer.train()\n",
    "trainer.save_model(\"models/test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch1.8",
   "language": "python",
   "name": "pytorch1.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
